# üñ•Ô∏è HETZNER SERVER SPECIFICATIONS & DEPLOYMENT STRATEGY
**Date**: November 29, 2025
**Server Type**: Hetzner Cloud VPS

---

## üìä CURRENT SERVER SPECIFICATIONS

### **CPU**
```
Processor: AMD EPYC-Rome
Cores: 8 vCPUs
Architecture: x86_64
Performance: ~7.5-8.5 GHz total (estimated)
```

### **RAM**
```
Total: 16 GB (15 GiB)
Used: 3.4 GB
Available: 11 GB
Swap: 0 GB (none configured)
Type: DDR4 ECC (estimated)
```

### **Storage**
```
Total: 38 GB
Used: 29 GB (81%)
Available: 7.2 GB
Type: NVMe SSD
Filesystem: ext4
```

### **GPU**
```
Status: NO GPU AVAILABLE ‚ùå
CUDA: Not supported
Inference: CPU-only
```

### **Network**
```
Bandwidth: Likely 1 Gbps (standard Hetzner)
Location: Unknown (likely EU datacenter)
IPv4/IPv6: Assumed available
```

---

## üéØ SERVER CLASSIFICATION

Based on specs, this is likely a **Hetzner CPX31 or CCX23** instance:

| Plan | vCPU | RAM | Storage | Price/mo |
|------|------|-----|---------|----------|
| **CPX31** | 8 vCPU | 16 GB | 240 GB | ‚Ç¨13.40 (~$14) |
| **CCX23** | 8 vCPU | 16 GB | 160 GB | ‚Ç¨30.51 (~$32) |

**Most Likely**: CPX31 (shared vCPU, cost-effective)

---

## üö® CRITICAL FINDINGS

### ‚ùå **NO GPU AVAILABLE**
This is a **CPU-only server** - no NVIDIA GPU for ML inference.

**Impact on Refactoring Plan:**
- ‚ùå Cannot self-host GPU models locally
- ‚ùå FLUX.1, Stable Diffusion XL require GPU
- ‚ùå Coqui TTS runs slowly on CPU (10-30s per clip)
- ‚úÖ **Must use Hugging Face Inference API**

### ‚ö†Ô∏è **LIMITED STORAGE (7.2 GB FREE)**
Current usage at 81% capacity.

**Impact:**
- Model downloads would fill disk quickly
  - FLUX.1-schnell: ~12 GB
  - Qwen2.5-72B: ~145 GB (quantized: ~36 GB)
  - Coqui XTTS-v2: ~1.8 GB
  - **Total**: ~50-160 GB needed
- ‚ùå **Cannot cache models locally**
- ‚úÖ **Must use HF Inference API (no local downloads)**

### ‚ö†Ô∏è **ADEQUATE RAM (11 GB AVAILABLE)**
Enough for workflow but not for large model inference.

**Impact:**
- ‚úÖ Sufficient for workflow orchestration
- ‚úÖ Can handle Redis caching
- ‚ùå Not enough for running 70B parameter models
  - Qwen2.5-72B needs ~80 GB RAM (quantized: ~40 GB)
  - FLUX.1 needs ~12 GB VRAM (GPU) or ~24 GB RAM (CPU)

### ‚úÖ **GOOD CPU (8 cores)**
Adequate for orchestration and API calls.

**Impact:**
- ‚úÖ Good for parallel workflow execution
- ‚úÖ Fast for orchestrator logic
- ‚ùå Too slow for ML model inference
  - CPU inference 10-50x slower than GPU
  - FLUX.1 on CPU: 2-5 minutes per image (vs 2-3 seconds on GPU)

---

## üéØ UPDATED DEPLOYMENT STRATEGY

### ~~Option A: Self-Hosted Models~~ **‚ùå NOT POSSIBLE**
**Reason**: No GPU, limited storage, insufficient RAM

### **Option B: Hugging Face Inference API (FREE)** ‚úÖ **RECOMMENDED**
**Why This Works Best:**

‚úÖ **No GPU required** - HF runs models on their infrastructure
‚úÖ **No storage needed** - No model downloads
‚úÖ **No RAM requirements** - Just API calls
‚úÖ **FREE tier available** - 1000+ requests/day
‚úÖ **Perfect for current server specs**

**Performance:**
- Image generation: 5-15 seconds (HF shared GPU)
- Text generation: 2-5 seconds
- Voice generation: 10-20 seconds (via API alternatives)

**Costs:**
- Free tier: 1000 requests/day
- Beyond free: ~$0.001-0.01 per request
- **Still 90%+ cheaper than current APIs**

### **Option C: Hugging Face PRO ($9/month)** ‚úÖ **ALSO VIABLE**
**Advantages over Free Tier:**

‚úÖ **Priority queue** - Faster inference (2-3x)
‚úÖ **Higher rate limits** - 10,000+ requests/day
‚úÖ **Better availability** - No queuing during peak times
‚úÖ **Still saves $352/year** vs current system

**When to Use:**
- If free tier is too slow
- If you need >1000 requests/day
- If reliability is critical

### ~~Option D: Upgrade Server with GPU~~ **‚ùå NOT RECOMMENDED**

Hetzner GPU servers (CCX42 with GPU):
- Price: ‚Ç¨179/month (~$190/month = $2,280/year)
- **Way more expensive** than HF API or even current system
- Overkill for this workload

---

## üí° OPTIMAL ARCHITECTURE FOR THIS SERVER

```
Hetzner CPX31 (8 vCPU, 16GB RAM, CPU-only)
         ‚Üì
OrchestratorAgent (runs on server)
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
Sub-Agents   API Calls
(local)      (remote)
    ‚îÇ         ‚îÇ
    ‚îú‚îÄ Data Acquisition (local: Airtable, Scraping)
    ‚îú‚îÄ Workflow Control (local: Redis, state mgmt)
    ‚îú‚îÄ Monitoring (local: metrics, logs)
    ‚îÇ
    ‚îî‚îÄ Content Generation ‚Üí Hugging Face API
         ‚îú‚îÄ Image Gen ‚Üí HF FLUX.1-schnell API
         ‚îú‚îÄ Text Gen ‚Üí HF Qwen2.5 API
         ‚îî‚îÄ Voice Gen ‚Üí HF Bark/Coqui API

    ‚îî‚îÄ Video Production ‚Üí Remotion (local)

    ‚îî‚îÄ Publishing ‚Üí Platform APIs (local calls)
         ‚îú‚îÄ YouTube API
         ‚îú‚îÄ WordPress API
         ‚îî‚îÄ Instagram API
```

**What Runs Locally:**
- ‚úÖ Orchestrator and all agents
- ‚úÖ Workflow state management
- ‚úÖ Redis caching
- ‚úÖ Remotion video rendering
- ‚úÖ API orchestration
- ‚úÖ File storage management

**What Uses External APIs:**
- üåê Hugging Face Inference API (images, text, voice)
- üåê ScrapingDog (Amazon scraping)
- üåê YouTube/WordPress/Instagram (publishing)

---

## üìä REVISED COST ANALYSIS

### **Current System**
```
OpenAI GPT-4o:     $0.10/video
GPT-4o Vision:     $0.05/video
fal.ai:            $0.21/video
ElevenLabs:        $0.10/video
ScrapingDog:       $0.02/video
TOTAL:             $0.48/video ($480/year)
```

### **New System - HF Free Tier**
```
HF FLUX.1 API:     $0.00 (free tier)
HF Qwen2.5 API:    $0.00 (free tier)
HF Voice API:      $0.00 (free tier)
ScrapingDog:       $0.02/video
TOTAL:             $0.02/video ($20/year)
SAVINGS:           $460/year (96%)
```

### **New System - HF PRO ($9/mo)**
```
HF PRO subscription: $9/month ($108/year)
ScrapingDog:         $0.02/video ($20/year)
TOTAL:               $128/year (1000 videos)
SAVINGS:             $352/year (73%)
```

### **New System - HF Pay-as-you-go**
If exceeding free tier without PRO:
```
HF Image Gen:      ~$0.03/video (7 images @ $0.004 each)
HF Text Gen:       ~$0.01/video
HF Voice Gen:      ~$0.02/video (via alternative API)
ScrapingDog:       $0.02/video
TOTAL:             ~$0.08/video ($80/year)
SAVINGS:           $400/year (83%)
```

**Recommendation**: Start with **HF Free Tier**, upgrade to **PRO** if needed.

---

## üöÄ IMPLEMENTATION PLAN UPDATES

### **Phase 1: Core Orchestrator** ‚úÖ **NO CHANGES**
Runs perfectly on 8 vCPU, 16GB RAM server.

### **Phase 2: Hugging Face Integration** ‚ö†Ô∏è **UPDATED**

**OLD Plan** (assumed GPU):
- Self-host FLUX.1, Qwen2.5, Coqui TTS
- Download models locally
- Run inference on local GPU

**NEW Plan** (CPU-only reality):
```python
# Use HF Inference API exclusively
from huggingface_hub import InferenceClient

class HuggingFaceImageGenerator:
    def __init__(self, config):
        self.client = InferenceClient(
            model="black-forest-labs/FLUX.1-schnell",
            token=config['hf_api_token']
        )
        # NO local model loading
        # NO GPU required
        # NO storage needed

    async def generate_image(self, prompt):
        # Call HF API directly
        result = await self.client.text_to_image(
            prompt=prompt,
            height=1920,
            width=1080
        )
        return result
```

**Key Changes:**
- ‚úÖ Use `InferenceClient` instead of local models
- ‚úÖ No model downloads (saves 50-160 GB storage)
- ‚úÖ No GPU requirements
- ‚úÖ Lightweight, fast deployment
- ‚ö†Ô∏è Depends on HF API availability (99.9% uptime)

### **Phase 3: Sub-Agents** ‚úÖ **NO MAJOR CHANGES**
All sub-agents run locally, just call HF API when needed.

### **Phase 4: Migration** ‚úÖ **NO CHANGES**
Works perfectly on current server.

---

## üîß CONFIGURATION UPDATES

### **Updated api_keys.json**
```json
{
  "// HUGGING FACE CONFIG": "=== Hugging Face Settings ===",
  "hf_api_token": "hf_xxxxxxxxxxxx",
  "hf_use_inference_api": true,
  "hf_use_local_models": false,
  "hf_pro_subscription": false,

  "// MODEL SELECTION": "=== Model IDs ===",
  "hf_image_model": "black-forest-labs/FLUX.1-schnell",
  "hf_text_model": "Qwen/Qwen2.5-72B-Instruct",
  "hf_voice_model": "suno/bark",

  "// PERFORMANCE TUNING": "=== API Settings ===",
  "hf_max_retries": 3,
  "hf_timeout": 60,
  "hf_concurrent_requests": 5,

  "// FALLBACK OPTIONS": "=== Keep during migration ===",
  "openai_api_key": "sk-...",
  "elevenlabs_api_key": "...",
  "fal_api_key": "..."
}
```

---

## ‚ö° PERFORMANCE EXPECTATIONS

### **Image Generation**
- **Current (fal.ai)**: 2-4 seconds per image
- **New (HF Free)**: 5-15 seconds per image
- **New (HF PRO)**: 3-6 seconds per image
- **Impact**: +10-30 seconds per video (7 images)

### **Text Generation**
- **Current (GPT-4o)**: 1-2 seconds
- **New (HF Qwen2.5)**: 2-5 seconds
- **Impact**: +3-15 seconds total

### **Voice Generation**
- **Current (ElevenLabs)**: 1-2 seconds per clip
- **New (HF Bark)**: 5-10 seconds per clip
- **Impact**: +20-40 seconds total (7 clips)

### **Total Workflow Time**
- **Current**: 3-5 minutes
- **New (HF Free)**: 4-7 minutes
- **New (HF PRO)**: 3.5-6 minutes
- **Impact**: +1-2 minutes (+30-40%)

**Trade-off**: 1-2 minutes slower for **96% cost savings** ‚úÖ **WORTH IT**

---

## üéØ RECOMMENDED NEXT STEPS

### **Immediate (Today)**
1. ‚úÖ Server specs confirmed (8 vCPU, 16GB RAM, no GPU)
2. [ ] Sign up for Hugging Face account
3. [ ] Get HF API token (free): https://huggingface.co/settings/tokens
4. [ ] Test HF Inference API with sample requests

### **This Week**
1. [ ] Implement HF API clients for image/text/voice
2. [ ] Test quality with 5 sample videos
3. [ ] Compare output quality vs current system
4. [ ] Measure actual performance times

### **Decision Point (End of Week)**
Based on testing results:
- If quality good + speed acceptable ‚Üí Use **HF Free Tier** ‚úÖ
- If quality good + speed too slow ‚Üí Upgrade to **HF PRO** ($9/mo)
- If quality poor ‚Üí Keep current APIs, reconsider architecture

### **Next 2 Weeks**
1. [ ] Build orchestrator framework
2. [ ] Create sub-agents with HF integration
3. [ ] Parallel deployment (old + new system)
4. [ ] Gradual migration

---

## üì¶ STORAGE OPTIMIZATION

Current usage: **29 GB / 38 GB (81%)**

**Recommendations:**
1. **Run cleanup immediately**
   ```bash
   python3 cleanup_all_storage.py
   ```
   Expected to free: 5-10 GB

2. **Enable automatic cleanup**
   - Weekly cleanup already scheduled (Sunday 7AM)
   - Consider daily cleanup if generating many videos

3. **Monitor storage daily**
   ```bash
   df -h / | grep sda1
   ```

4. **Consider storage upgrade if needed**
   - CPX31 with 240 GB: +‚Ç¨1.80/month
   - Or clean up old files more aggressively

---

## ‚úÖ FINAL RECOMMENDATIONS

### **Deployment Strategy**
üéØ **Use Hugging Face Inference API (Free Tier)**

**Why:**
- ‚úÖ No GPU required (perfect for CPU-only server)
- ‚úÖ No storage needed (7 GB free is enough)
- ‚úÖ No RAM pressure (11 GB free is sufficient)
- ‚úÖ 96% cost savings ($460/year)
- ‚úÖ Simple implementation (just API calls)
- ‚úÖ Works perfectly on current Hetzner CPX31

**If Free Tier is Too Slow:**
- Upgrade to **HF PRO** ($9/month)
- Still saves **$352/year** (73% vs current)
- 2-3x faster inference
- Higher reliability

### **Server Upgrades**
‚ùå **Do NOT upgrade server** - current specs are perfect for API-based architecture

### **Architecture**
‚úÖ **Proceed with Orchestrator + Sub-Agent design**
- Local orchestration on Hetzner server
- Remote inference via HF API
- Best of both worlds

---

## üìû SUPPORT

**Questions?**
1. How to get HF API token?
   - Visit: https://huggingface.co/settings/tokens
   - Create token with "Inference API" permissions

2. How to test HF API?
   ```bash
   pip install huggingface_hub
   python3 test_hf_api.py
   ```

3. What if HF is down?
   - Keep current APIs as fallback during migration
   - HF has 99.9% uptime SLA

---

**Last Updated**: November 29, 2025
**Server**: Hetzner CPX31 (8 vCPU, 16GB RAM, CPU-only)
**Recommendation**: ‚úÖ Hugging Face Inference API (Free ‚Üí PRO if needed)
**Expected Savings**: 96% ($460/year)
**Performance Impact**: +1-2 minutes per video (acceptable)
